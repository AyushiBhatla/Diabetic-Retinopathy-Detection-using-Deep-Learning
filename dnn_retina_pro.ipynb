{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "593555c8-4075-420f-aaa1-43c123e03206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, LSTM, Reshape, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "449ac25b-4a55-4c3c-ad12-405abb5e97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224  # Adjust this based on your dataset\n",
    "BATCH_SIZE = 32\n",
    "INPUT_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "EPOCHS = 200# Adjust number of epochs based on your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1cea5ad-023b-4023-a643-f190aff32bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "INPUT_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "EPOCHS = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7e27045-fb2d-45d5-8361-453dc62048a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train path exists: C:/Users/hp/Desktop/MTECH_ SEM 2/DNN/Diagnosis of Diabetic Retinopathy/train\n",
      "Test path exists: C:/Users/hp/Desktop/MTECH_ SEM 2/DNN/Diagnosis of Diabetic Retinopathy/test\n",
      "Found 2076 images belonging to 2 classes.\n",
      "Found 231 images belonging to 2 classes.\n",
      "Number of classes: 2\n",
      "Class labels: {'DR': 0, 'No_DR': 1}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set image size and batch size\n",
    "IMG_SIZE = (224, 224)  # You can change this to fit your model\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# Define paths to train and test directories (ensure the paths are correct)\n",
    "# Update the paths to the absolute location\n",
    "train_path = 'C:/Users/hp/Desktop/MTECH_ SEM 2/DNN/Diagnosis of Diabetic Retinopathy/train'\n",
    "test_path = 'C:/Users/hp/Desktop/MTECH_ SEM 2/DNN/Diagnosis of Diabetic Retinopathy/test'\n",
    "\n",
    "\n",
    "# Check if the directories exist (optional, for debugging)\n",
    "if not os.path.exists(train_path):\n",
    "    print(f\"Train path does not exist: {train_path}\")\n",
    "else:\n",
    "    print(f\"Train path exists: {train_path}\")\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    print(f\"Test path does not exist: {test_path}\")\n",
    "else:\n",
    "    print(f\"Test path exists: {test_path}\")\n",
    "\n",
    "# Initialize ImageDataGenerator\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load training data\n",
    "train_data = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=IMG_SIZE,       # Resize images to IMG_SIZE\n",
    "    batch_size=BATCH_SIZE,      # Define batch size\n",
    "    class_mode='categorical'    # Multi-class classification\n",
    ")\n",
    "\n",
    "# Load testing data\n",
    "test_data = datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=IMG_SIZE,       # Resize images to IMG_SIZE\n",
    "    batch_size=BATCH_SIZE,      # Define batch size\n",
    "    class_mode='categorical',   # Multi-class classification\n",
    "    shuffle=False               # Make sure test data isn't shuffled\n",
    ")\n",
    "\n",
    "# Number of classes (in your case it should be 2: DR and NO_DR)\n",
    "NUM_CLASSES = train_data.num_classes\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "\n",
    "# Optional: View class labels\n",
    "print(\"Class labels:\", train_data.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98bb5c0a-22f8-4b8f-b919-58e073b22eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- MODEL BUILDING -------------------\n",
    "def build_cnn():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=INPUT_SHAPE),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_dnn():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=INPUT_SHAPE),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_rnn():\n",
    "    model = Sequential([\n",
    "        Reshape((IMG_SIZE, IMG_SIZE * 3), input_shape=INPUT_SHAPE),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        LSTM(64),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_lenet():\n",
    "    model = Sequential([\n",
    "        Conv2D(6, (5, 5), activation='relu', input_shape=INPUT_SHAPE),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(16, (5, 5), activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(120, activation='relu'),\n",
    "        Dense(84, activation='relu'),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_vgg16():\n",
    "    base = VGG16(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
    "    for layer in base.layers:\n",
    "        layer.trainable = False\n",
    "    x = base.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    model = Model(inputs=base.input, outputs=preds)\n",
    "    return model\n",
    "\n",
    "def build_resnet():\n",
    "    base = ResNet50(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
    "    for layer in base.layers:\n",
    "        layer.trainable = False\n",
    "    x = base.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    preds = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    model = Model(inputs=base.input, outputs=preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ccfe72-e6e1-4d12-8f4c-8e102013993a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training CNN WITHOUT Optimizer =====\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense, Dropout,\n",
    "                                     LSTM, Reshape, Input, BatchNormalization,\n",
    "                                     GlobalAveragePooling2D)\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "\n",
    "# ------------------- GLOBAL CONFIG -------------------\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "INPUT_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "EPOCHS = 200\n",
    "\n",
    "# These should be defined from your dataset\n",
    "# Example:\n",
    "# train_data = ImageDataGenerator(...).flow_from_directory(...)\n",
    "# test_data = ImageDataGenerator(...).flow_from_directory(...)\n",
    "NUM_CLASSES = train_data.num_classes\n",
    "\n",
    "# ------------------- MODEL BUILDING -------------------\n",
    "def build_cnn():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=INPUT_SHAPE),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_dnn():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=INPUT_SHAPE),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_rnn():\n",
    "    model = Sequential([\n",
    "        Reshape((IMG_SIZE, IMG_SIZE * 3), input_shape=INPUT_SHAPE),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        LSTM(64),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_lenet():\n",
    "    model = Sequential([\n",
    "        Conv2D(6, (5, 5), padding='same', activation='relu', input_shape=INPUT_SHAPE),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(16, (5, 5), activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(120, activation='relu'),\n",
    "        Dense(84, activation='relu'),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_vgg16():\n",
    "    base = VGG16(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
    "    for layer in base.layers:\n",
    "        layer.trainable = False\n",
    "    x = Flatten()(base.output)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    return Model(inputs=base.input, outputs=preds)\n",
    "\n",
    "def build_resnet():\n",
    "    base = ResNet50(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
    "    for layer in base.layers:\n",
    "        layer.trainable = False\n",
    "    x = GlobalAveragePooling2D()(base.output)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    return Model(inputs=base.input, outputs=preds)\n",
    "\n",
    "# ------------------- TRAINING SETUP -------------------\n",
    "models = {\n",
    "    \"CNN\": build_cnn,\n",
    "    \"DNN\": build_dnn,\n",
    "    \"RNN\": build_rnn,\n",
    "    \"LeNet\": build_lenet,\n",
    "    \"VGG16\": build_vgg16,\n",
    "    \"ResNet\": build_resnet\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    \"adam\": Adam,\n",
    "    \"sgd\": SGD,\n",
    "    \"rmsprop\": RMSprop\n",
    "}\n",
    "\n",
    "learning_rates = [0.001, 0.01]\n",
    "results = []\n",
    "history_logs = {}\n",
    "model_summaries = {}\n",
    "speed_logs = {}\n",
    "\n",
    "# ------------------- CONFUSION MATRIX PLOT -------------------\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'Confusion Matrix - {title}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------- TRAIN & EVALUATE -------------------\n",
    "def train_and_evaluate(model_fn, optimizer_fn, lr, model_name, label):\n",
    "    model = model_fn()\n",
    "    model_summaries[f\"{model_name}_{label}\"] = [layer.__class__.__name__ for layer in model.layers]\n",
    "\n",
    "    if optimizer_fn:\n",
    "        optimizer = optimizer_fn(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(train_data, epochs=EPOCHS, validation_data=test_data, verbose=0)\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    loss, acc = model.evaluate(test_data, verbose=0)\n",
    "    history_logs[f\"{model_name}_{label}\"] = history\n",
    "    speed_logs[f\"{model_name}_{label}\"] = duration\n",
    "\n",
    "    preds = model.predict(test_data)\n",
    "    y_pred = np.argmax(preds, axis=1)\n",
    "    y_true = test_data.classes\n",
    "    class_labels = list(test_data.class_indices.keys())\n",
    "\n",
    "    plot_confusion_matrix(y_true, y_pred, class_labels, f\"{model_name}_{label}\")\n",
    "    print(f\"Classification Report - {model_name}_{label}:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_labels))\n",
    "\n",
    "    return loss, acc, model\n",
    "\n",
    "# ------------------- MAIN TRAINING LOOP -------------------\n",
    "trained_models = {}\n",
    "\n",
    "for model_name, model_fn in models.items():\n",
    "    print(f\"===== Training {model_name} WITHOUT Optimizer =====\")\n",
    "    try:\n",
    "        loss, acc, model = train_and_evaluate(model_fn, None, None, model_name, \"none\")\n",
    "        results.append((model_name, \"None\", \"None\", loss, acc))\n",
    "        trained_models[f\"{model_name}_none\"] = model\n",
    "        print(f\"‚úÖ {model_name} no optimizer - Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error in {model_name} without optimizer: {e}\")\n",
    "\n",
    "    for opt_name, opt_class in optimizers.items():\n",
    "        for lr in learning_rates:\n",
    "            print(f\"===== Training {model_name} with {opt_name.upper()} (lr={lr}) =====\")\n",
    "            try:\n",
    "                loss, acc, model = train_and_evaluate(model_fn, opt_class, lr, model_name, f\"{opt_name}_{lr}\")\n",
    "                results.append((model_name, opt_name, lr, loss, acc))\n",
    "                trained_models[f\"{model_name}_{opt_name}_{lr}\"] = model\n",
    "                print(f\"‚úÖ {model_name} + {opt_name} - Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error in {model_name} with {opt_name} (lr={lr}): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896827e2-8e0e-40aa-b3c9-2135a5e6a2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Plot individual Accuracy and Loss graphs per model\n",
    "for key, history in history_logs.items():\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val')\n",
    "    plt.title(f'Accuracy - {key}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Val')\n",
    "    plt.title(f'Loss - {key}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Merged Accuracy & Loss Graphs\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Merged Accuracy\n",
    "for key, history in history_logs.items():\n",
    "    plt.plot(history.history['val_accuracy'], label=f'{key}')\n",
    "plt.title('Validation Accuracy Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Merged Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "for key, history in history_logs.items():\n",
    "    plt.plot(history.history['val_loss'], label=f'{key}')\n",
    "plt.title('Validation Loss Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Create summary report table\n",
    "summary_df = pd.DataFrame(results, columns=['Model', 'Optimizer', 'Learning Rate', 'Loss', 'Accuracy'])\n",
    "summary_df = summary_df.sort_values(by='Accuracy', ascending=False)\n",
    "summary_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"\\nüìã Model Performance Summary:\")\n",
    "display(summary_df)\n",
    "\n",
    "# Optional: Save to CSV\n",
    "# summary_df.to_csv('model_summary_report.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a8ff0-9902-4309-b8e5-bf17a7e2dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Plot Accuracy & Loss per Model\n",
    "# -------------------------------\n",
    "for key, history in history_logs.items():\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val')\n",
    "    plt.title(f'Accuracy - {key}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Val')\n",
    "    plt.title(f'Loss - {key}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Merged Accuracy Graph (only)\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(14, 7))\n",
    "for key, history in history_logs.items():\n",
    "    plt.plot(history.history['val_accuracy'], label=key)\n",
    "plt.title('üìà Validation Accuracy Comparison Across Models')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right', fontsize='small')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Summary Table\n",
    "# -------------------------------\n",
    "summary_df = pd.DataFrame(results, columns=['Model', 'Optimizer', 'Learning Rate', 'Loss', 'Accuracy'])\n",
    "summary_df = summary_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nüìã Model Performance Summary:\")\n",
    "display(summary_df)\n",
    "\n",
    "# Optional: Save report\n",
    "# summary_df.to_csv(\"model_accuracy_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48919f1d-51b6-41ec-9b36-99bd5835208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load a sample image\n",
    "# -------------------------------\n",
    "img_path = r\"C:\\Users\\hp\\Desktop\\MTECH_ SEM 2\\DNN\\Diagnosis of Diabetic Retinopathy\\test\\DR\\362c4a96cebb_png.rf.192f091fc13737d8c432a2b9c16bd035.jpg\"\n",
    "\n",
    "img = image.load_img(img_path, target_size=(224, 224))  # Resize to match model input\n",
    "\n",
    "# Show image\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Sample Input Image\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Preprocess the image\n",
    "# -------------------------------\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0) / 255.0  # Normalize if needed\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Predict using a trained model\n",
    "# -------------------------------\n",
    "model_name = 'CNN_adam_0.001'  # use any available trained model\n",
    "model = trained_models[model_name]\n",
    "\n",
    "pred = model.predict(img_array)\n",
    "predicted_class = np.argmax(pred, axis=1)[0]\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Display Prediction\n",
    "# -------------------------------\n",
    "class_labels = list(test_data.class_indices.keys())  # example: ['dr', 'no_dr']\n",
    "predicted_label = class_labels[predicted_class]\n",
    "\n",
    "print(f\"‚úÖ Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e47702-8260-4945-b0a6-9d68d6f923b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load the No_DR image\n",
    "# -------------------------------\n",
    "img_path = r\"C:\\Users\\hp\\Desktop\\MTECH_ SEM 2\\DNN\\Diagnosis of Diabetic Retinopathy\\valid\\No_DR\\4d167ca69ea8_png.rf.ba34a6446a9941030c1b8601d0211fb1.jpg\"\n",
    "\n",
    "img = image.load_img(img_path, target_size=(224, 224))  # Resize to match model input\n",
    "\n",
    "# Show image\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Sample Input Image (No_DR)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Preprocess the image\n",
    "# -------------------------------\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0) / 255.0  # Normalize\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Loop through all models to predict\n",
    "# -------------------------------\n",
    "class_labels = list(test_data.class_indices.keys())  # ['dr', 'no_dr']\n",
    "\n",
    "# Predict with all models\n",
    "for model_name, model in trained_models.items():\n",
    "    pred = model.predict(img_array)\n",
    "    predicted_class = np.argmax(pred, axis=1)[0]\n",
    "    predicted_label = class_labels[predicted_class]\n",
    "    print(f\"Model: {model_name} - Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721827bb-fb3a-43cf-a1ad-a21284f065c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23763e8-bb75-4398-b68c-f85edd85dc48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225070e4-9595-4659-8e5a-641490bd81e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b931ed-914c-4298-a1c5-c846df95483c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda89f1d-6778-481f-b052-57ecb131c8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4c2bdb-625c-4fdf-821f-e4cb7b6c059b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a361bc6-50e4-4ae1-87c8-ae497066b5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a59965-c2dc-439e-ab8a-c56277b4181e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c32999b-4bd3-4d34-9751-756e1f0daca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df2c01-3b8b-4f3d-b533-856b5d1d6ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb8c24-bb2d-4f0a-b902-fee8d85d3b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dnn)",
   "language": "python",
   "name": "dnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
